---
layout: post
title:  从贝叶斯到高斯判别分析
date:   2017-11-08 19:41:00
img: 
description: 看着似乎懂，却又似乎哪里还不清楚，就是这种棉花肚的感觉。不过网上也有很多关于贝叶斯的精彩博文。我还是第一次用这种书写文字来整理思路的方式学习。
---

（竖线不知怎的会消失，条件概率中的竖线都换成了斜线）

概率里常见的问题是：（参考：https://www.guokr.com/post/801946/）

假设：（1）两个碗里混杂地放着饼干，碗A和碗B。（1）饼干分两种口味，饼M和饼N。
那么，想从碗A里拿到饼干M，有两步要做选择：（1）先拿到碗A的概率p(A)，再从碗A里拿到饼干M的概率p(M/A)；（2）先闭着眼拿到饼干M的概率p(M)，再看是不是从碗A里拿的概率p(A/M)。
当然，肯定是p(M/A)p(A)=p(A/M)p(M)

虽然“肯定是”，但心理上尚觉不舒服，总感觉哪里没理顺。

也可以这样理解：（1）饼干从碗里拿出来的情形：饼干已经放在碗里了，然后计算从碗A里拿出饼干M的概率；（2）饼干放到碗里去的情形：碗还是空着的，要把饼干放进去，然后计算饼干M放到碗A里的概率。

这种一进一出的可逆想法，也许通过路线选择的模型来理解更容易接受些。比如从广州到上海有两种路线，上海到北京有三种路线，那么按照从广州-上海-北京的顺序计算一条实际出行路线，跟北京-上海-广州的是一样的。最终只要得到这条想要的路线就好了。

或者不用返程，就用路线分段制定，我先计划广州-上海的路线，再计划上海-北京的路线；跟先计划后者，再计划前者，结果是一样的。

所以饼干问题也可以这样理解：有一块原味的饼干，要把它变成小明碗里的巧克力口味的饼干，要分两段走，有两种做法：（1）先将原味饼干抹上巧克力酱，再放到小明碗里；（2）先把原味饼干放到小明碗里，再抹上巧克力酱。

这样的举例模型理解起来顺畅多了。以前学概率论的时候，就经常把各种问题转为一些简单的例子。贝叶斯定理还是用往返行程的路线举例比较容易想舒服。

饼干问题：比如有两种饼干（口味改成颜色），红色r和黑色b的；有两种碗0和1。也可以理解为每一块饼干有两种属性（颜色、碗归属），把碗归属的属性再换成饼干的形状，一种饼干是小狗，一种是大象，代码分别也是0和1。

于是饼干有两种属性（形状，颜色）。上面的问题就变成了我先取出小狗饼干，再从小狗饼干里取出红色的；还是先取出红色的饼干，再从红色饼干里取出小狗模样儿的。（感觉这样理解更舒服）

比如5块小狗，其中红2个，黑3个；4块大象，其中红3个，黑1个。或者：

5块红色，其中狗2个，象3个；4块黑色，其中狗3个，象1个。

0 r

0 r

0 b

0 b

0 b

1 r

1 r

1 r

1 b

不要饼干了，就直接说动物。假设有一批生物，外形是小狗的生物都是黄色的，外形是大象的生物都是灰色的，那么数据是：

0 y

0 y

0 y

0 y

0 y

1 g

1 g

1 g

1 g

这里的外形与颜色之间有一定的关系，从外形可以知道颜色，从颜色可以知道外形，这就像是个函数。如果我们知道0，通过函数f，可以获知y，即f(0)=y；f(1)=g。即0->f->y；1->f->g。这个f就像是个传递中间信号的使者一样，很像神经元。所以函数就是神经元，它们的功能（function）都是传递。

这个神经元当然也可以是别的函数，比如：大象100%都是灰色的，小狗80%是黄色的。

0 y

0 y

0 y

0 y

0 g

1 g

1 g

1 g

1 g

晚上黑黑的看不到颜色，如果看到小狗的身影，我们知道它80%是黄色的；看到大象的身影，肯定它是灰色的。

如果我们近视，看不清动物形状大小，但能看出是黄色的，那它肯定是小狗；如果看出是灰色的，那么有80%的可能是大象。

这种推断过程，就是函数，就是神经元。其实还真的就是我们神经元反馈出来的认知信息。


动物的形状是分类，颜色也是分类，都只有两种数值。现在换成连续型的数据，颜色换成鼻子长度。小狗和大象还是用0和1表示（不说外形了，直接指两种动物），鼻子长短用具体的数值（瞎编的），当然大象的长一点，搞个100倍的样子吧。

数据这么看：

5只小狗，鼻子长1~5；4只大象，鼻子长1~104。

那么，反过来怎么看？直观地看，好像鼻子短的是小狗，长的是大象。

0 1

0 2

0 3

0 4

0 5

1 101

1 102

1 103

1 104

其实我们可以从整个实数范围来看，小狗的鼻子长度在[1,5]中出现了5次，除此之外的其他实数范围内出现0次；大象的鼻子长度在[101,104]中出现了4次，其他实数上出现0次。这就是次数分布。除以总数以后可以换算成比例，就是次数的百分比分布，也就是出现的概率分布。小狗的100%分布在[1,5]，大象的100%分布在[101,104]。所以这里的神经元就是概率分布函数。



一本正经的“高斯判别分析”：

设：

x表示鼻子的长度，符合高斯分布；

y表示动物类别，0是狗，1代表大象，符合伯努利分布。

判别学习算法：

训练数据时，根据鼻子的长度x，判别是哪种动物y，并画条函数分割线。（即求x->y的函数）。

预测新数据时，把鼻子长度代入函数，看结果属于分割线的哪边。

例如：

求得的函数f: x=[1,5]时，y=0；x=[101.104]时，y=1。

那么预测x=2.5时，根据f，就有y=0，是小狗。

生成学习算法：

训练数据时，先看清是哪种动物p(y)，然后算每种动物的鼻子长度的概率分布p(x/y)。（即在已知小狗和大象的前提下，对它们各自不同的鼻子长度模型建立了两种函数）。
预测新数据时，看新动物鼻子的属于哪种鼻子模型，是p(x/y=0)的概率分布，那就是小狗；是p(x/y=1)的概率分布的，那就是大象。
例如：

求得的y=0时的f范围是3±2；y=1时的f范围是102.5±1.5。

那么预测x=2.5时，根据x属于y=0时的f，那么它就是小狗。


#################不同点解析#################

对待同一批训练数据时，判别学习算法直接求得x到y的函数f(x)（比如这里其实是S型函数）。在求函数f(x)过程中，先求得这个函数的概率分布p(f(x))。因为这个函数是通过一个x传递出y的预测值，而当预测值等于期望均方时，也就是概率最大的时候。

所以当p(f(x))概率最大时，求得其参数，参数定了，函数f(x)也就定了。

这里的p(f(x))也可表示为p(y/x)。定义函数f(x)的参数是θ，也可写作f(x;θ)，那么其概率可表示为p(y/x;θ)。

思路总结：先假定两个数据集X和Y符合某个函数f，因为是假定的，所以也可以把函数f表示为假设h。虽然假设了大致的映射关系，但不知道具体的参数，比如线性关系，那也得知道具体是哪条线，所以目标就是求函数（假设）的参数。

即，第1步：假设y=h(x)；第2步，求得其概率分布p(h(x))，即p(y/x)，带上参数表示为p(y/x;θ)。第3步，借助训练数据x和y，不断调整参数θ，当概率p趋于最大时，这时候的θ值就定了。那么h(x)也定了。


而生成学习算法先把y分好类，且知道y取值的概率分布类型p(y)，比如分两类的就是二元伯努利分布。再求得每一类y中x的概率分布p(x/y)。由此获得联合概率p(x,y)。而贝叶斯说p(x/y)p(y)=p(x,y)=p(y/x)p(x)。所以在这里，我们想求p(y/x)的最大值，不是通过计算它本身，而是计算p(x/y)p(y)的最大值。

####这里不对####（我们要求的是p最大时的θ值，也相当于是h(x)值，即y值，所以p(x)作为一个与h无关的非负数可以无视。注：牢记θ确定h预测y，三个东西其实是一回事儿的。）。####

所以，第1步，假设y符合某个分布（比如伯努利），然后算出它的概率分布p(y)；再假设每个y类别里的x符合某个分布（比如高斯），也计算出它们的概率分布p(x/y=0)和p(x/y=1)。第2步，借助训练数据x和y，不断调整参数，使联合概率p(x/y)p(y)趋于最大，联合概率最大时的参数，就是我们需要的。（因为联合概率最大也就是让p(h(x))最大，虽然这里h不曾出现）。

区别总结：判别法直接求p(y/x)最大时的参数；生成法求p(x/y)p(y)最大时的参数。

当x符合高斯分布（一般样本量较大才能看出来）时，求p(x/y)好，可以用GDA。当x不符合或不确定是不是高斯分布时（样本量低的大多数情况下），用逻辑回归。

因为x的概率分布是其他分布，y的概率分布也是逻辑函数。而y的概率分布是逻辑函数时，x的概率分布是可以多种多样的。

换言之，生成法中的GDA，在建模的时候既假设了y的概率分布是逻辑函数，也假定了x的概率分布是高斯分布。它的模型假设更强，所以效果更好（如果x的概率分布能确定的话）。


因为y是分类问题的函数，所以如果先做y分类，假定了p(y)，就叫class prior“类的先验”（先于抽样前就知道的分布）。然后通过训练样本的计算，得到了p(y/x)，这就是后验分布了（抽样后计算得到的）。


理解概率表达式：

在判别法中，p(y/x)就是训练组中的y的概率分布p(y)，期望均方是h(x)，所以与x和h（也就是参数θ）的变化有关。注意：与σ无关，因为每个样都一样。

也就是说，这里的y的期望均方不是有y的训练数据集决定的，而是由假定的函数得出的y的预测值决定的。其实这里求的是误差值的概率分布。所以p(y/x)其实就是p(ε)。

在生成法中，

p(y)是训练组中的y的概率分布，期望均方只跟y有关，与x等无关。

也就是说，这里的y的期望均方，真是就是训练数据集里的y的期望均方。而不是误差值p(ε)的概率分布了。

p(x/y)可以看做p(f(x))，是每个y类别中的x的概率分布。 