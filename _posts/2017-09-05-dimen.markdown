---
layout: post
title:  "数据维度和降维"
date:   2017-09-05 09:21:00
img: 
description: “怎么就没人说人话”，这句话指的就是降维。//狗血的生活费啊。。。
---

数据的维度就是X轴、就是自变量、就是实验设计中的因素。这么简单对接不同知识点的话，网上就是没人会说。总算看了几个例子明白了。但是作为一张n*m维度的数据表，它的维度就是n与m的乘积？这该如何想象还是不舒服。但数据点就像星团中的星星，主要还是三维空间。但n*m实际上只有两因素（虽然水平数很高），当因素增加，那可能就不是三维空间的事情了。

降维大概就是在数据点构成的星团所在的空间放一块二维的平面，把所有的点的影子都标出来，有的算法尽量让点分开（如PCA保持原数据信息），有的尽量聚拢（如拉普拉斯特征映射反映数据内在的流形结构）。
不同算法：http://saili.science/2017/06/04/dimension-reduction/
流形结构：我简单的理解为空间如果是个弯曲的三角形，那么映射后也要表现出三角形，而不是其他形状。（参见“流形”wiki）


扩展阅读-数据降维和特征选取：
"
二者的目标都是使得特征维数减少。但是方法不一样。
1. 数据降维，一般说的是维数约简（Dimensionality reduction）。它的思路是：将原始高维特征空间里的点向一个低维空间投影，新的空间维度低于原特征空间，所以维数减少了。在这个过程中，特征发生了根本性的变化，原始的特征消失了（虽然新的特征也保持了原特征的一些性质）。
2. 特征选择，是从 n 个特征中选择 d (d<n) 个出来，而其它的 n-d 个特征舍弃。所以，新的特征只是原来特征的一个子集。没有被舍弃的 d 个特征没有发生任何变化。这是二者的主要区别。
作者：Jason Gu
链接：https://www.zhihu.com/question/29262795/answer/43742530
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
"

"
主要区别在于坐标上。特征选择，是在给定一组特征（也即确定了坐标）后，从中选取一个特征子集，因此相当于把一些坐标去除的过程。在大部分情况下特征选择都是在冗余变量较多的情况下使用，此时相当于坐标为斜坐标，甚至还存在冗余坐标（即用p个特征表达了k (k<p)维空间），因此删除冗余坐标并不会显著降维。另一方面，若原来的特征本身就是正交坐标系，那么删除多少特征就降了多少维，此时与降维类似，只是这种降维方式限定了只通过删除某几个坐标轴来实现。降维，如果特指PCA这种线性降维方法，则降维所得的子空间是在原始坐标系旋转下不变的。而如果坐标系恰好选取为主向量，则PCA实际上等价于对这组特殊的坐标系进行特征选择，方式是根据样本在坐标轴上分散的程度来决定该坐标轴的去留。而在一般情形下，PCA降维所得的子空间是由几乎所有原始特征张成的，因此原始特征全部起作用。因此，有学者（Zou & Hastie)提出了sparse PCA，旨在强迫使用部分原始特征张成尽量“优质”的子空间，同时实现了降维+特征选择，从而能在分析主成分的同时还可以加入模型的解释性。如果涉及到非线性降维，如流形学习等方法，则与特征选择区别甚大，此时不仅有降维，还带有坐标轴的非线性扭转伸缩等操作。特征选择在更多情形下，还只是限于线性的范畴（此处的线性指对参数的线性）。
作者：过拟合
链接：https://www.zhihu.com/question/29262795/answer/43928542
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
"

"
我们的学习是什么，学习的本质是什么？其实在我看来就是一种特征抽取的过程，在学习一门新知识的时候，这里一个知识点，那儿一个知识点，你头脑里一篇混乱，完全不知所云，这些知识点在你的大脑中也纯粹是杂乱无章毫无头绪的，这不正是高维空间里数据的特征么？最本质的数据完全湮没在太多太多的扰动中，而我们要做的就是提炼，从一堆毫无头绪的扰动中寻找到最本质的真理。
作者：weizier
链接：https://www.zhihu.com/question/29262795/answer/45254386
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
"

"
监督和无监督：
所以，你一定要在无监督文本上筛选特征，那么目前看来比较好的方法是放弃feature selection，而用feature extraction, 也就是上图中的feature transformation.
链接：https://www.zhihu.com/question/23455769/answer/40800820
"
“
1.从数据设置上看：
无监督学习：训练样本数据和待分类的类别已知，但训练样本数据皆为非标签数据；
监督学习：训练样本数据和待分类的类别已知，且训练样本数据皆为标签数据；半监督学习：训练样本数据和待分类的类别已知，然而训练样本既有标签数据，也有非标签数据；
2.典型方法：
无监督学习：k-聚类、主成分分析等；
监督学习：支持向量机、线性判别；   半监督学习：S3VM、S4VM、CS4VM、TSVM；
半监督学习的方法的例子举得不恰当，毕竟半监督学习的方法有很多的分类，我具的几个例子[2]都是将半监督学习思想引用到SVM算法发展起来的咯。[1]杨伟. 半监督学习方法研究[D]. 国防科学技术大学, 2011.[2]年素磊. 半监督支持向量机综述[不知道啊].南京大学

作者：昕露
链接：https://www.zhihu.com/question/27138263/answer/70955755
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
”

监督学习是带有标签的，如一堆照片上表明了猫、狗等（训练样本数据）。可进一步分为回归和分类两类问题。分类的输出变量属于一个范畴，回归的输出变量是一个实值，还有一些种类的问题建立在分类和回归之上，包括推荐问题和时序预测。



============================

生活方面：
几番问询后，使馆今天告诉我生活费还要等到下个月25号发，也就是说还有50天。看到邮件就火冒三丈，正在培训的Safe Induction也根本听不进了。当时就想骂娘。回家做午饭的时候还想着给所领导、院领导写信，丫的扣每个月5000的岗位津贴、年终绩效降低、来年公积金降低，每个月两千的出国补助只给单位公派的人，明摆着其他公派的就不是亲生的；院里说好的预支一个月生活费也没有兑现，而国家公派是预支三个月的，出国后三个月大使馆再发三个月生活费。现在我这边居然也是第3个月再发。前期已经垫了各种费用，包括全年的保险费。现在居然还要预支3个月生活费，那么回国时再多领3个月的生活费有意思吗？就是带回国换人民币吗？这一来一回不要说手续费的损失，而且人民币在逐步升值，这样搞最后损失还不知道多少。而且这边办了银行卡，首个月存钱利息还高1.5%，这点点滴滴的加起来，亏损巨大，我出国一趟图什么？当然也不是在哭穷，老家好歹也有两套不值钱的老房子，但总不至于这么点资金周转不灵就让人卖房吧。35岁的现金流压力有多大他们知不知道，为什么还像看小孩一样不给零花钱！如果把人送出去却不能让人安心学习，反而不断的心寒，这样收得住人心吗？很不爽啊！反正助研就是流水的兵，时间到了年老色衰了非升即走，换批年轻的照样卖苦力。

平静下来以后，尤其是化悲愤为力气做了100个俯卧撑之后，决定忍下来。但生活消费刷国内信用卡也算了，房租转账怎么整？信用卡又不能转账，只好跟室友用等额人民币兑换了，到时候等拿到生活费再换回来，计算了一下还是损失余额宝的利息亏得少一些。

每次出国都感叹国外的体系很完善，包括这次实验室培训，各方面都有人负责，都有科学的规范，而国内实验室连最小的一点点破事都能拖上很久，更不要说安全、合理的规范。就像这次院公派的不合理，中科院的公派出国须知就写得不负责任，所里也落井下石（扣岗位津贴是觉得出国就不是在为所里工作了？还是觉得有了额外的公派补贴？就不想想国外的消费水平？根据我的估算，我要想达到跟去年一样的收支差额，我的吃穿用度开销就得挤压到300澳元/月），然后大使馆也死板、不会急人所急。这从小的说是体系，往大了说就是体制。目前国家是强大了，但什么时候能开始有建立体系的概念，不要总是跟乡巴佬暴发户一样。